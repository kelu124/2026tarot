{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02cd7755",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "493b9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3435d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ mrg_behav.parquet.gzip already exists, skipping...\n",
      "✓ mrg_concern.parquet.gzip already exists, skipping...\n",
      "✓ mrg_issue.parquet.gzip already exists, skipping...\n",
      "✓ articles.parquet.gzip already exists, skipping...\n",
      "✓ mrg_tech.parquet.gzip already exists, skipping...\n",
      "⬇ Downloading seeds.parquet.gzip... ✓ Done (2.17 MB)\n",
      "\n",
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "files = [\"https://github.com/kelu124/Futures/raw/refs/heads/main/data/mrg_behav.parquet.gzip\",\n",
    "\"https://github.com/kelu124/Futures/raw/refs/heads/main/data/mrg_concern.parquet.gzip\",\n",
    "\"https://github.com/kelu124/Futures/raw/refs/heads/main/data/mrg_issue.parquet.gzip\",\n",
    "\"https://github.com/kelu124/Futures/raw/refs/heads/main/data/articles.parquet.gzip\",\n",
    "\"https://github.com/kelu124/Futures/raw/refs/heads/main/data/mrg_tech.parquet.gzip\",\n",
    "\"https://github.com/kelu124/Futures/raw/refs/heads/main/data/seeds.parquet.gzip\"\n",
    "\n",
    "]\n",
    "\n",
    "output_dir = Path(\"files\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "for url in files:\n",
    "    # Extract filename from URL\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    filepath = output_dir / filename\n",
    "    \n",
    "    # Check if file already exists\n",
    "    if filepath.exists():\n",
    "        print(f\"✓ {filename} already exists, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Download the file\n",
    "    print(f\"⬇ Downloading {filename}...\", end=\" \")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Write to file\n",
    "        with open(filepath, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        file_size = filepath.stat().st_size / (1024 * 1024)  # Size in MB\n",
    "        print(f\"✓ Done ({file_size:.2f} MB)\")\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"✗ Failed: {e}\")\n",
    "\n",
    "print(\"\\nDownload complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f4c178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading parquet files...\n",
      "\n",
      "Converting to JSON format...\n",
      "\n",
      "Saving to combined_data.json...\n"
     ]
    }
   ],
   "source": [
    "# Read the parquet files\n",
    "print(\"Reading parquet files...\")\n",
    "\n",
    "df = pd.read_parquet(output_dir / \"articles.parquet.gzip\")\n",
    "df = df.fillna(\"\")\n",
    "lst = df[df.origin.str.startswith(\"2025\")][\"file_name\"].to_list()\n",
    "df5 = pd.read_parquet(output_dir / \"seeds.parquet.gzip\")\n",
    "df5 = df5[df5.src.isin(lst)]\n",
    "df1 = pd.read_parquet(output_dir / \"mrg_behav.parquet.gzip\")\n",
    "df2 = pd.read_parquet(output_dir / \"mrg_concern.parquet.gzip\")\n",
    "df3 = pd.read_parquet(output_dir / \"mrg_issue.parquet.gzip\")\n",
    "df4 = pd.read_parquet(output_dir / \"mrg_tech.parquet.gzip\")\n",
    "\n",
    "df1 = df1.dropna()\n",
    "df2 = df2.dropna()\n",
    "df3 = df3.dropna()\n",
    "df4 = df4.dropna()\n",
    "df1 = df1[df1.relevancy >= 4]\n",
    "df2 = df2[df2.relevancy >= 4]\n",
    "df3 = df3[df3.relevancy >= 4]\n",
    "df4 = df4[df4.relevancy >= 4]\n",
    "df5 = df5[df5.relevancy >= 4]\n",
    "\n",
    "# Convert dataframes to JSON format\n",
    "print(\"\\nConverting to JSON format...\")\n",
    "data = {\n",
    "    'behav': df1[df1.src.isin(lst)].to_dict(orient='records'),\n",
    "    'concern': df2[df2.src.isin(lst)].to_dict(orient='records'),\n",
    "    'issue': df3[df3.src.isin(lst)].to_dict(orient='records'),\n",
    "    'tech': df4[df4.src.isin(lst)].to_dict(orient='records'),\n",
    "    'seeds': df5.to_dict(orient='records')\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "output_file = \"combined_data.json\"\n",
    "print(f\"\\nSaving to {output_file}...\")\n",
    "with open(\"docs/data/\"+output_file, 'w') as f:\n",
    "    json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ee8949c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['name', 'description', 'relevancy', 'src'], dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
